package org.pengfei.Lesson06_Spark_Streaming

import java.io.{BufferedReader, InputStream, InputStreamReader}

import org.apache.log4j.receivers.net.SocketHubReceiver
import org.apache.log4j.{Level, Logger}
import org.apache.spark.SparkConf
import org.apache.spark.rdd.RDD
import org.apache.spark.sql.SparkSession
import org.apache.spark.storage.StorageLevel
import org.apache.spark.streaming.dstream.DStream
import org.apache.spark.streaming.flume._
import org.apache.spark.streaming.kafka.KafkaUtils
import org.apache.spark.streaming.{Minutes, Seconds, StreamingContext}
import org.apache.spark.streaming.twitter._

import scala.collection.mutable

object Lesson06_1_Spark_Streaming {

  def main(args:Array[String])= {
    Logger.getLogger("org").setLevel(Level.OFF)
    Logger.getLogger("akka").setLevel(Level.OFF)
    val master="local[2]"
    val appName="Lesson6_1_Spark_Streaming"
    // you can also set appName in chinese
    val chineseAppName="宋体"

    val spark=SparkSession.builder().appName(chineseAppName).master(master).getOrCreate()


    // DStream Example
    DStreamExample(spark)

  }




  /*************************************************************************************************
    * ***************************************6.1. Spark Streaming Introduction***************************
    * **********************************************************************************************/
/*
* Batch processing systems have high latency. Depending on the volume of data, it may take anywhere
* from a few minutes to a few hours to process a batch. Some organizations run nightly batch processing jobs
* that run for 6 to 12 hours on a cluster of hundreds of machines. Thus, there is a long wait before you can see
* the results generated by a batch processing application. In addition, since data is not immediate processed,
* the gap between the time when data is collected and the time when the batch processing result becomes
* available is even longer. This time gap is acceptable for a certain class of applications.
*
* However, sometimes data needs to be processed and analyzed as it is collected. For example, fraud
* detection in an e-commerce system must happen in real time. Similarly, network intrusion or security
* breach detection must be in real time. Another example is application or device failure detection in a data
* center. To prevent a long downtime, data must be processed right away.
*
* One of the challenges with live data stream processing is handling high-velocity data in real time or
* near real time. A data stream processing application running on a single machine will not be able to handle
* high-velocity data. A distributed stream processing framework addresses this issue.
*
* In this class, we introduces Spark Streaming. The introduction is followed by a detailed discussion of the
* application programming interface provided by Spark Streaming. At the end of the lesson, you will develop
* an application using Spark Streaming.
*
* The spark official doc can be found: https://spark.apache.org/docs/latest/streaming-programming-guide.html */

  /****************************************What is spark Streaming *****************************************/

  /*
  * Spark Streaming is a distributed data stream processing framework. It makes it easy to develop distributed
* applications for processing live data streams in near real time. It not only provides a simple programming
* model but also enables an application to process high-velocity stream data. It also allows the combining of
* data streams and historical data for processing.
*
* Spark Streaming is a spark add-on (a spark library that runs on top of spark). It extends Spark for data
* steam processing. It provides higher-level abstractions for processing steaming data, but under the hood,
* it uses Spark. Spark streaming lib inherits all the features and benefits of Spark core.
*
* In addition, it can be used along with other Spark libraries, such as Spark sql, ML, MLlib, GraphX.
* Thus, spark steaming makes the power of the complete spark stack available for processing data streams*/

  /*************************************************************************************************
    * ***************************************6.1.1 Spark Streaming Architecture***************************
    * **********************************************************************************************/

  /**********************************Basic mechanism*******************************************/

  /* Spark Streaming processes a data stream in micro-batches. It splits a data stream into batches of very small
fixed-sized time intervals. Data in each micro-batch is stored as an RDD, which is then processed using
Spark core (see Figure 6-2 page 80). Any RDD operation can be applied to an RDD created by Spark Streaming. The
results of the RDD operations are streamed out in batches.*/

  /*********************************Accepted Data source for spark steaming********************************/

  /*
  * Spark Streaming supports a variety of data stream sources, including TCP socket, Twitter, Kafka, Flume,
* Kinesis, ZeroMQ, and MQTT. It can also be used to process a file as a stream. In addition, you can extend it to
* process data from a custom streaming data source.
*
* The streaming data sources for which Spark Streaming has built-in support can be grouped into two
* categories:
* - basic sources
* - advanced sources.
*
* - Basic data stream sources : include TCP sockets, Akka Actors, and files. Spark Streaming includes the libraries
*                               required to process data from these sources. A Spark Streaming application that wants
*                               to process data streams from a basic source needs to link only against the Spark
*                               Streaming library.
*
* - Advanced data stream sources : include Kafka, Flume, Kinesis, MQTT, ZeroMQ, and Twitter. The libraries required
*                                  for processing data streams from these sources are not included with Spark Streaming,
*                                  but are available as external libraries. A Spark Streaming application that wants
*                                  to process stream data from an advanced source must link against not only the Spark
*                                  Streaming library, but also the external library for that source.
*
* */

  /*************************************************************************************************
    * ***************************************6.1.2 Spark Streaming key components***************************
    * **********************************************************************************************/

  /**********************************************Receiver********************************************/
  /* A Receiver receives data from a streaming data source and stores it in memory. Spark Streaming creates and
* runs a Receiver on a worker node for each data stream. An application can connect to multiple data streams
* to process data streams in parallel.*/

  /*********************************************Destinations*******************************************/
  /* The results obtained from processing a data stream can be used in a few different ways (see Figure 6-3, page 81).
* The results may be fed to another application, which may take some action or just display it. For example, a
* Spark Streaming application may feed the results to a dashboard application that is updated continuously.
* Similarly, in a fraud detection application, the results may trigger cancellation of a transaction. The results
* can also be stored in a storage system such as a file or a database.*/


  /************************************************************************************************************
    * *********************6.2 Spark Streaming Application programming Interface (API)***************************
    * ******************************************************************************************************/

    def SteamingContextExample():Unit={
  /* To use spark streamin, we need to add a dependency in maven : In the below example, we use spark-streaming 2.3.1
   *  with scala version 2.11
  * <dependency>
  *  <groupId>org.apache.spark</groupId>
  *  <artifactId>spark-streaming_2.11</artifactId>
  *  <version>2.3.1</version>
  * </dependency>
  */

  /********************************6.2.1 Spark Streaming context ******************************/
  /* Spark streaming entry point is StreamingContext, the below example shows how to create a StreamingContext
  * from scratch. */


    val master="local[2]"
    val appName="Lesson6_1_Spark_Streaming"
    val batchInterval=10
    val conf =new SparkConf().setAppName(appName).setMaster(master)
    val ssc=new StreamingContext(conf,Seconds(batchInterval))

    /* master value can be a:
    * - spark standalone cluster url
    * - spark Mesos cluster url
    * - spark yarn cluster url
    * - local[*] local model
    *
    * you will not want to hardcode master in the program, but rather launch the application with spark-submit and
    * receive it there. However, for local testing and unit tests, you can pass “local[*]” to run Spark Streaming
    * in-process (detects the number of cores in the local system). Note that this internally creates a SparkContext
    * (starting point of all Spark functionality) which can be accessed as ssc.sparkContext. */

    /* We can also create a Streaming context from an existing context. The below example shows how to create a
    * Streaming Context from a sparkContext/sparkSession */

    //val spark=SparkSession.builder().appName(appName).master(master).getOrCreate()
    //val ssc1=new StreamingContext(spark.sparkContext,Seconds(batchInterval))

    /* Seconds() defines the batch interval of the streaming context(micro batch). It must be set based on the
    * latency requirements of your application and available cluster resources. A data stream is split into batches
    * of this time duration and each batch is processed as anRDD.
    *
    * The smallest batch interval is 500 milliseconds. The upper bound is determined by the latency requirements of
    * your applicaiton and available memory. The executors created for a Spark Streaming application must have
    * sufficient memory to store the received data in memory for good performance.
    *
    * For a Spark Streaming application running on a cluster to be stable, the system should be able to process data
    * as fast as it is being received. In other words, batches of data should be processed as fast as they are
    * being generated. Whether this is true for an application can be found by monitoring the processing times
    * in the streaming web UI, where the batch processing time should be less than the batch interval.
    *
    * Depending on the nature of the streaming computation, the batch interval used may have significant impact on
    * the data rates that can be sustained by the application on a fixed set of cluster resources. For example,
    * let us consider a WordCountNetwork application example. For a particular data rate, the system may be able
    * to keep up with reporting word counts every 2 seconds (i.e., batch interval of 2 seconds), but not every
    * 500 milliseconds. So the batch interval needs to be set such that the expected data rate in production can
    * be sustained.
    *
    * A good approach to figure out the right batch size for your application is to test it with a conservative
    * batch interval (say, 5-10 seconds) and a low data rate. To verify whether the system is able to keep up
    * with the data rate, you can check the value of the end-to-end delay experienced by each processed batch
    * (either look for “Total delay” in Spark driver log4j logs, or use the StreamingListener interface).
    * If the delay is maintained to be comparable to the batch size, then system is stable. Otherwise,
    * if the delay is continuously increasing, it means that the system is unable to keep up and it
    * therefore unstable. Once you have an idea of a stable configuration, you can try increasing the data
    * rate and/or reducing the batch size. Note that a momentary increase in the delay due to temporary data
    * rate increases may be fine as long as the delay reduces back to a low value (i.e., less than batch size).
    * */

    /******************************6.2.2 Spark Streaming application key stages ******************************/

    /* After a context is defined, you have to do the following key stages
    * 1. - Define the input sources by creating input DStreams.
    * 2. - Define the streaming computations by applying transformation and output operations to DStreams.
    * 3. - Start receiving data and processing it using streamingContext.start(). Check point can be set after running
    * 4. - Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().
    * 5. - The processing can be manually stopped using streamingContext.stop().
    *
    * Points to be remembered:
    * - Once a context has been started, no new streaming computations can be set up or added to it.
    * - Once a context has been stopped, it cannot be restarted.
    * - Only one StreamingContext can be active in a JVM at the same time.
    * - stop() on StreamingContext also stops the SparkContext. To stop only the StreamingContext, set the optional
    *   parameter of stop() called stopSparkContext to false.
    * - A SparkContext can be re-used to create multiple StreamingContexts, as long as the previous StreamingContext
    *   is stopped (without stopping the SparkContext) before the next StreamingContext is created.
    *   */


  /*****************************  Starting Steam Computation**************************************/
  ssc.start()
    /* The start method begins stream computation. Nothing really happens in a Spark steaming application
    * until the start method is called on an instance of the StreamingContext class. A spark streaming applicaiton
    * begins receiving data after it calls the start method. */

    /**********************************CheckPointing************************************************/

    /* The checkpoint method defined in the streamingContext class tells Spark Streaming to periodically checkpoint
    * data. It takes the name of a directory as an argument. For a production application, the checkpoint directory
    * should be on a fault-tolerant storage system such as HDFS.*/
    val checkPointDir="/tmp/spark-streaming/check-point"
    ssc.checkpoint(checkPointDir)

    /* A Spark Streaming application must call this method if it needs to recover from driver failures or if it
     * performs stateful transformations. The data processed by a Spark Streaming application is conceptually
     * a never ending sequence of continuous data. If the machine running the driver program crashes after
     * some data has been received but before it has been processed, there is a potential for data loss. Ideally,
     * a Spark Streaming application should be able to recover from failures without losing data. To enable this
     * functionality, Spark Streaming requires an application to checkpoint metadata.
     *
     * In addition, data checkpointing is required when an application performs stateful transformation on a
     * data stream. A stateful transformation is an operation that combines data across multiple batches in a data
     * stream. An RDD generated by a stateful transformation depends on the previous batches in a data stream.
     * Therefore, the dependency tree for an RDD generated by a stateful transformation grows with time. In case
     * of a failure, Spark Streaming reconstructs an RDD using its dependency tree. As the dependency tree grows,
     * the recovery time increases. To prevent recovery time from becoming too high, Spark Streaming checkpoints
     * intermediate RDDs of a stateful transformation. Therefore, Spark Streaming requires an application to call
     * the checkpoint method prior to using a stateful transformation. The stateful transformations supported by
     * Spark Streaming are discussed later in this Lesson.*/

    /***********************************Stopping Stream Computation***********************************/

    /* The stop method, as the name implies, stops stream computation. By default, it also stops SparkContext.
* This method takes an optional parameter that can be used to stop only the StreamingContext, so that the
* SparkContext can be used to create another instance of StreamingContext.
*
* stop(boolean stopSparkContext, boolean stopGracefully). Stop the execution of the streams, with option of ensuring
*          all received data has been processed.
*          stopSparkContext - if true, stops the associated SparkContext. The underlying SparkContext will be stopped
*          regardless of whether this StreamingContext has been started.
*          stopGracefully - if true, stops gracefully by waiting for the processing of all received data to be completed
*          By default, they are set to be true. */

    // stop only the streaming context, not the spark context
    ssc.stop(true)

    /*******************************Waiting for Stream computation to finish*****************************/

    /* The awaitTermination method in the StreamingContext class makes an application thread wait for stream
     * computation to stop.
     *
     * The awaitTermination method is required if a driver application is multi-threaded and the start
     * method was called not from the main application thread but by another thread. The start method in the
     * StreamingContext class is blocking method; it does not return until stream computation is finished or
     * stopped. In a single-threaded driver, the main thread will wait until the start method returns. However,
     * if the start method was called from another thread, you can prevent your main thread from exiting
     * prematurely by calling awaitTermination.*/

    ssc.awaitTermination()
    }
    /*****************************************************************************************************
      ************************************ 6.2.3 Steaming Context data type *****************************
      * ***************************************************************************************************/
def DStreamExample(spark:SparkSession):Unit={
  val batchInterval=10
  val ssc=new StreamingContext(spark.sparkContext,Seconds(batchInterval))

  val hostName:String="localhost"
  val port:Int=9999
  // Check point can be also write on hdfs,ssc.checkpoint("/user/hadoop/checkpoint")
  val checkPointPath="/tmp/spark/check-point"
  ssc.checkpoint(checkPointPath)
    /******************************************6.2.3.1 Discretized Streams(DStreams)*****************************/
    /* DStream is the basic abstraction provided by Spark Streaming. It represents a continuous stream of data,
    * either the input data stream received from source, or the processed data stream generated by transforming
    * the input stream. Internally, a DStream is represented by a continuous series of RDDs,
    * Each RDD in a DStream contains data from a certain interval (figure 6-4, page 85).
    *
    * DStream is defined as an abstract class in the Spark Streaming library. It defines an interface for
    * processing a data stream. Spark Streaming provides concrete classes that implement the DStream interface
    * for stream data from a variety of sources. I use the term DStream generically to refer to both the abstract
    * class and the classes implementing the interface defined by the DStream class.
    *
    * Since a DStream is a sequence of RDDs, it inherits the key RDD properties. It is immutable, partitioned, and
    * fault tolerant.
    * */

    /****************************************Creating a DStream ******************************************/

    /* A DStream can be created from a streaming data source or from an existing DStream by applying a
     * transformation. Since DStream is an abstract class, you cannot directly create an instance of the DStream
     * class. The Spark Streaming library provides factory methods for creating instances of the concrete classes
     * that implement the interface defined by DStream.
     *
     * We can get a DStream from a DataSource.
     * - Basic DataSource
     *   -- socketStream
     *   -- socketTextStream
     *   -- fileStream
     *   -- actorStream
     *   -- queueSteam
     *
     * - Advance DataSource
     *   -- Kafka
     *   -- Flume
     *   -- Kinesis
     *
     * Points to remember
     *   When running a Spark Streaming program locally, do not use “local” or “local[1]” as the master URL. Either of
     *   these means that only one thread will be used for running tasks locally. If you are using an input DStream
     *   based on a receiver (e.g. sockets, Kafka, Flume, etc.), then the single thread will be used to run the
     *   receiver, leaving no thread for processing the received data. Hence, when running locally, always use
     *   “local[n]” as the master URL, where n > number of receivers to run (see Spark Properties for information
     *   on how to set the master).
     *
     *   Extending the logic to running on a cluster, the number of cores allocated to the Spark Streaming application
     *   must be more than the number of receivers. Otherwise the system will receive data, but not be able to
     *   process it.
     *
     * */

/**********************************************************************************************************
  * *********************************Basic data source************************************************
  * ****************************************************************************************************/

  /*The following code examples are basic data source in the spark streaming api*/

  /*************************************socketTextStream************************************************/
 //  SocketTextStreamExample(ssc,hostName,port)
  /***************************************socketStream*****************************************************/
// SocketStreamExample(ssc,hostName,port)

  /**********************************rawSocketStream *****************************************************/
// RawSocketStreamExampl(ssc,hostName,port)

  /**********************************TextFileStream***********************************/
 // TextFileStreamExample(ssc)
  /***********************************Actor Stream*******************************************/
//ActorStreamExampl(ssc)
  /***************************************Queue Stream*************************************/
//QueueStreamExampl(ssc)

  /**********************************************************************************************************
    * *********************************Advance data source************************************************
    * ****************************************************************************************************/

  /* The factory methods for creating a DStream from advanced sources such as Kafka, Flume, or Twitter are not
   * built-in, but available through extra utility classes. To process a data stream from an advanced source, an
   * application needs to perform the following steps:
   *
   * 1. Import the utility class for that source and create a DStream using the factory
   *    method provided by that class.
   * 2. Link against the library that contains the utility class for that source.
   * 3. Create an uber JAR that includes all application dependencies and deploy the application on a Spark cluster.*/

  /*The following code examples are advance data source in the spark streaming api*/

  /**********************************TwitterStream**************************************/
  //TwitterStreamExample(ssc)
  /*************************************FlumeStream******************************************/
  //FlumeStreamExample(ssc,hostName,port)
  /*****************************************Kafka Steam ******************************************/
  //KafkaStreamExample(ssc)
}
  def SocketTextStreamExample(ssc:StreamingContext,hostName:String,port:Int):Unit={
    /*************************************socketTextStream************************************************/

    /* The socketTextStream method creates a DSteam that represents stream data received over a TCP socket connection.
    * It takes three input parameters. The first argument is the hostname of the data source. The second argument
    * is the port to connect to for receiving data. The third argument, which is optional, specifies the storage
    * level for the received data.
    *
    * The default value of the storageLevel is StorageLeve1.MEMORY_AND_DISK_SER_2. which stores the received data
    * first in memory and spills to disk if the available memory is insufficient to store all received data. In addition,
    * it deserializes the received data and reserializes it using Spark’s serialization format. Thus, this storage level
    * incurs the overhead of data serialization, but it reduces JVM garbage collection-related issues. The received
    * data is also replicated for fault tolerance.*/

    val lines:DStream[String] = ssc.socketTextStream(hostName,port,StorageLevel.MEMORY_ONLY)
    /*lines is a DSteam of String which represents the stream of data that will be received. */
    // Split each lines into words
    val words=lines.flatMap(_.split(" "))
    // simple word count
    val pairs=words.map(word=>(word,1))
    val wordCounts=pairs.reduceByKey(_ + _)
    wordCounts.print()
    ssc.start()
    ssc.awaitTermination()
  }

  /************************Socket Stream********************************************************************/

  def SocketStreamExample(ssc:StreamingContext,hostName:String,port:Int):Unit={
   /* socketStream takes four arguments, 1st is hostname:String, 2nd is port:Int, 3rd is converter:scala.Function, 4th is
     * storageLevel.*/
    val lines=ssc.socketStream(hostName,port,linesHash,StorageLevel.MEMORY_ONLY)
    // The linesHash is an example of the converter function which takes InputStream as input and return a Iterator as
    // output
    lines.print()
    ssc.start()
    ssc.awaitTermination()
  }

  /**************************************rawSocketStream******************************************/

  def RawSocketStreamExampl(ssc:StreamingContext,host:String,port:Int):Unit={

val numStreams=3
    /**
      * Receives text from multiple rawNetworkStreams and counts how many '\n' delimited
      * lines have the word 'the' in them. This is useful for benchmarking purposes. This
      * will only work with spark.streaming.util.RawTextSender running on all worker nodes
      * and with Spark using Kryo serialization (set Java property "spark.serializer" to
      * "org.apache.spark.serializer.KryoSerializer").
      * Usage: RawNetworkGrep <numStreams> <host> <port> <batchMillis>
      *   <numStream> is the number rawNetworkStreams, which should be same as number
      *               of work nodes in the cluster
      *   <host> is "localhost".
      *   <port> is the port on which RawTextSender is running in the worker nodes.
      *   <batchMillise> is the Spark Streaming batch duration in milliseconds.
      */
//For each stream nums, we create a rawSocketStream
val rawStreams=(1 to numStreams).map(_=>ssc.rawSocketStream[String](host,port,StorageLevel.MEMORY_AND_DISK_SER_2)).toArray
 // we union the Dstream of the three rawSocketStream
    val union=ssc.union(rawStreams)
    //union represent the union of the data received from the three socket
    union.print()
    union.filter(_.contains("the")).count().foreachRDD(r =>
      println("Grep count: " + r.collect().mkString))
    ssc.start()
    ssc.awaitTermination()

  }

  /*******************************************TextFileStream*************************************************/
  def TextFileStreamExample(ssc:StreamingContext):Unit={
    val dirPath="/tmp/spark-streaming/file_stream_test"
    /* The textFileStream method creates a DStream that monitors a Hadoop-compatible file system for new files
    * and reads them as text files. It takes a directory path as input to monitor. Files must be written to the
    * monitored directory by moving them from another location within the same file system. For example, on a
    * Linux system, files should be written into the monitored directory using the mv command.*/

  val lines=ssc.textFileStream(dirPath)
    val wordCount=lines.flatMap(line=>line.split(" ")).map(word=>(word,1)).reduceByKey((x,y)=>x+y)
    wordCount.print()
  }

  /***************************************Actor Stream*********************************************/
  def ActorStreamExampl(ssc:StreamingContext):Unit={
    /* The actorStream method creates a DStream with a user-implemented Akka actor Receiver.*/


  }

  /***************************************Queue( of RDD) Stream*********************************************/
  def QueueStreamExampl(ssc:StreamingContext):Unit={
    /* The Queue objects implement data structures that allow to insert and retrieve elements in a first-in-first-out
    * (FIFO) manner*/
    val rddQueue = new mutable.Queue[RDD[Int]]()
    val inputStream=ssc.queueStream(rddQueue)

    val mappedStream=inputStream.map(x=>(x%10,1))
    val reducedStream = mappedStream.reduceByKey(_ + _)
    reducedStream.print()
    ssc.start()

    //create and push some RDDs into rddQueue
    for(i<-1 to 30){
      rddQueue.synchronized{
        rddQueue += ssc.sparkContext.makeRDD(1 to 1000, 10)
      }
      Thread.sleep(1000)
    }

    ssc.stop()
  }

/****************************************************** Twitter Stream Example ***************************************/
  def TwitterStreamExample(ssc:StreamingContext):Unit={
    /*System.setProperty("twitter4j.oauth.consumerKey","consumerKey")
    System.setProperty("twitter4j.oauth.consumerSecret","consumerSecret")
    System.setProperty("twitter4j.oauth.accessToken",accessToken)
    System.setProperty("twitter4j.oauth.accessTokenSecret", "accessTokenSecret")*/
    TwitterUtils.createStream(ssc,None)
  }

  /***********************************************FlumeStreamExample************************************************/
  def FlumeStreamExample(ssc:StreamingContext,host:String,port:Int):Unit={
    val stream = FlumeUtils.createStream(ssc, host, port, StorageLevel.MEMORY_ONLY_SER_2)
    // Print out the count of events received from this server in each batch

    stream.count().map(cnt => "Received " + cnt + " flume events." ).print()

    val mappedlines = stream.map{sparkFlumeEvent =>
      val event = sparkFlumeEvent.event
      println("Value of event " + event)
      println("Value of event Header " + event.getHeaders)
      println("Value of event Schema " + event.getSchema)
      val messageBody = new String(event.getBody.array())
      println("Value of event Body " + messageBody)
      messageBody}.print()
    ssc.start()
    ssc.awaitTermination()

  }
/********************************************Kafka Stream Example*********************************************/
  //Not tested
  def KafkaStreamExample(ssc:StreamingContext):Unit={
    //Kafka runs on top of ZooKeeper, so we are actually doing stream on zookeeper
    val zkQuorum = "hadoop-nn.bioaster.org:2181" //Zookeeper server url
    // config Kafka group and topic
    val group = "1"  //set topic group, for example val group = "test-consumer-group"
    val topics = "Hello-Kafka"  //topics name
    val numThreads = 3  //set topic partition number
    val topicMap =topics.split(",").map((_,numThreads.toInt)).toMap
    // create kafkaStream
    val lineMap = KafkaUtils.createStream(ssc,zkQuorum,group,topicMap)
    val lines = lineMap.map(_._2)
    val words = lines.flatMap(_.split(" "))
    val pair = words.map(x => (x,1))
    val wordCounts = pair.reduceByKeyAndWindow(_ + _,_ - _,Minutes(2),Seconds(10),2)
    wordCounts.print
    ssc.start
    ssc.awaitTermination

  }

  /*
  * An iterator is not a collection(list, array, etc.) but a way to access the elements of a collection one by one.
  * The basic operation of it is next and hasNext. Calling next will return the next element of the current iterator
  * position, if there are not more elements to return, it will throw NoSuchElementException. As it is a traversal
  * pointer into a collection. It can be used only once*/
  def linesHash(inputStream:InputStream):Iterator[(String,String)]={
    val dataInputStream = new BufferedReader(new InputStreamReader(inputStream,"UTF-8"))
    dataInputStream.read().toString.map(x=>(x.toString,x.hashCode().toString)).iterator

  }
}