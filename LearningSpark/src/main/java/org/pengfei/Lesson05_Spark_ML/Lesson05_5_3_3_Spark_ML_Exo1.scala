package org.pengfei.Lesson05_Spark_ML

import org.apache.log4j.{Level, Logger}
import org.apache.spark.ml.Pipeline
import org.apache.spark.ml.classification.LogisticRegression
import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator
import org.apache.spark.ml.feature.{HashingTF, Tokenizer}
import org.apache.spark.ml.param.ParamMap
import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions._

object Lesson05_5_3_3_Spark_ML_Exo1 {
  def main(args:Array[String])={
    Logger.getLogger("org").setLevel(Level.OFF)
    Logger.getLogger("akka").setLevel(Level.OFF)
    val spark = SparkSession.builder().master("local[2]").appName("Lesson5_5_3_3_Spark_ML_Exo1").getOrCreate()

    /****************************************************************************************************************
      * **********************************5.5.3.3 Spark ML Exo1*****************************************************
      * ***********************************************************************************************************/

    /************************************5.5.3.3.1 data set *********************************************************/

    /* We will use the sentiment labeled sentences dataset available at https://archive.ics.uci.edu/ml/
     * datasets/Sentiment+Labelled+Sentences. It was created for the paper, “From Group to Individual Labels
     * Using Deep Features” published by Dimitrios Kotzias, Misha Denil, Nando de Freitas, and Padhraic Smyth at
     * KDD 2015.
     *
     * This dataset contains a sample of reviews from three websites, imdb.com, amazon.com, and yelp.com.
     * It includes randomly selected 500 positive and 500 negative reviews from each website. A review with
     * negative sentiment is labeled 0 and positive review is labeled 1. A review is separated from its label
     * by the tabcharacter.
     *
     * For simplicity sake, we will use only the reviews from imdb.com. These reviews are in the
     * imdb_labelled.txt file.*/

    /************************************5.5.3.3.2 Goal of the exo *************************************************/

    /* Our goal is to train a predictive model that predicts whether a sentence has a positive or negative sentiment.
    * To be more specific, we will train and evaluate a binary classifier using the dataset in the
    * imdb_labelled.txt file.*/
    import spark.implicits._

    /*******************Step 1 read data and transform data into dataframe********************/
    val filePath="/home/pliu/data_set/spark_data_set/spark_lessons/Lesson5_Spark_ML/sentiment_labelled_sentences/imdb_labelled.txt"
    val lines=spark.sparkContext.textFile(filePath)
    val totoalLineNum=lines.count()
    println(s"Total line number : $totoalLineNum")
    val word=lines.map(line=>line.split("\\t"))
    //println(s"word value: ${word.first().toArray.mkString(";")} ")
    val df=word.map{a=>ImdbDS(a(0),a(1).toDouble)}.toDF()

    // understand data
    // get data size
    val rowNum=df.count()
    val columnNum= df.columns.length
    println(s"data set has $rowNum rows, and $columnNum columns"  )

    // get data schema
    df.printSchema()

    // get all possible label value (for classification problem)
    df.select($"label").distinct().show()

    // get observation number or each label
    df.groupBy("label").agg(count($"text").alias("textNumCount")).show()

    // test if text column has null value
val nullText=df.filter($"text".isNull||$"text".isNaN).count()
    val nullLabel=df.filter($"label".isNull||$"label".isNaN).count()
    println(s"null text number is $nullText , null label number is $nullLabel")
   // get a dataset sample
     df.show(5)

    /********************************Step 2 prepare test, training data***********************************/
    val Array(trainingData,testData)=df.randomSplit(Array(0.8,0.2))
    println(s"test data size ${testData.count()}, training data size ${trainingData.count()}")

    /*******************************Step 3 Transform data to a format which ml can use **********************/
   // tokenize training data
    val tokenizer = new Tokenizer().setInputCol("text").setOutputCol("words")
    val tokenizedData= tokenizer.transform(trainingData)
    // create a feature vector to represent a sentence
    /* HashingTF is a Transformer that converts a sequence of words into a fixed-length feature Vector. It maps
     * a sequence of terms to their term frequencies using a hashing function.
     *
     * The preceding code first creates an instance of the HashingTF class. Next, it sets the number of features,
     * the name of the input DataFrame column that should be used for generating feature Vectors, and the name
     * for the new column that will contain the feature Vectors generated by HashingTF.*/
    val hashingTF=new HashingTF()
      .setNumFeatures(1000)
      .setInputCol(tokenizer.getOutputCol)
      .setOutputCol("features")
    val hashedData=hashingTF.transform(tokenizedData)
    /* Now we have the Transformers required to transform our raw data into a format that can be used with a
     * machine learning algorithm. The DataFrame that will be generated by the transform method of hashingTF
     * will have a column named "label", which stores each label as a Double, and a column named "features",
     * which stores the features for each observation as a Vector.
     *
     * Next, we need an Estimator to fit a model on the training dataset. For this example, we will use the
     * LogisticRegression class provided by the Spark ML library*/
    /*******************************Step 4 build a ml model ****************************************/
val lrModel= new LogisticRegression()
      .setMaxIter(10)
      .setRegParam(0.01)

    /******************************Step 5 build a pipeline *****************************************/
    /* We can build a pipeline which can chained the transformer and ml model togethor, this can avoid many
    * intermediate values*/
    val pipeline=new Pipeline().setStages(Array(tokenizer,hashingTF,lrModel))
    /* The above pipeline creates an instance of the Pipeline class with three stages. The first two stages are
     * Transformers and third stage is an Estimator. The pipeline object will first use the specified Transformers to
     * transform a DataFrame containing raw data into a DataFrame with the feature Vectors. Finally, it will use the
     * specified Estimator to train or fit a model on the training dataset.*/
    /*****************************Step 6 train the pipeline model ***********************************/
    val pipelineModel=pipeline.fit(trainingData)
   /******************************Step 7 predict the training data************************************/
    val testPredictions=pipelineModel.transform(testData)
    val traiingPredictions=pipelineModel.transform(trainingData)
    /*****************************Step 8 evaluate the model *****************************************/
    val evaluator = new BinaryClassificationEvaluator()
    val evaluatorParamMap=ParamMap(evaluator.metricName->"areaUnderROC")
    val aucTraining=evaluator.evaluate(traiingPredictions,evaluatorParamMap)
    val aucTest=evaluator.evaluate(testPredictions,evaluatorParamMap)

    println(s"auc for training is $aucTraining, for test is $aucTest")
    /* we can noticed that our auc score for training data is 1.0 which means the model is prefect, but for test
    * data, it only has 0.71, which is not far from 0.5 (worthless model)*/
    /***************************Step 9 tuning the model**********************************/
  /*To improve a models performance, we often tune the model's hyperparameters first. Spark Ml provides
  * a CrossValidator class that can help with this task. It requires a parameter grid over which it conducts
  * a grid search to find the best hyperparameters using k-fold cross validation*/
val paramGrid = new ParamGridBuilder()
      .addGrid(hashingTF.numFeatures,Array(10000,10000))
        .addGrid(lrModel.regParam,Array(0.01,0.1,1.0))
        .addGrid(lrModel.maxIter,Array(20,30))
          .build()

    /* This code created a parameter grid consisting of two values for the number of features, three values for
* the regularization parameters, and two values for the maximum number of iterations. It can be used to do
* a grid search over 12 different combinations of the hyperparameter values. You can specify more options,
* but training a model will take longer since grid search is a brute-force method that tries all the different
* combinations in a parameter grid. As mentioned earlier, using a CrossValidator to do a grid search can be
* expensive in terms of CPU time.*/

    val crossValidator=new CrossValidator()
      .setEstimator(pipeline)
      .setEstimatorParamMaps(paramGrid)
        .setNumFolds(10)
      .setEvaluator(evaluator)

    val crossValidatorModel=crossValidator.fit(trainingData)

    /* The fit method in the CrossValidator class returns an instance of the CrossValidatorModel class.
* Similar to other model classes, it can be used as a Transformer that predicts a label for a given feature Vector*/

    val newTestPredictions = crossValidatorModel.transform(testData)

    val newAucTest = evaluator.evaluate(newTestPredictions, evaluatorParamMap)

    println(s"auc value with cross validation tuning is ${newAucTest}")

    /* As you can see, the performance of the cross validation model is much better with a auc score of  0.825*/
  }
case class ImdbDS(text:String,label:Double)
}
